{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:08:37.206296Z",
     "start_time": "2019-12-23T22:08:34.426333Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import shutil\n",
    "import subprocess\n",
    "import shlex\n",
    "import json\n",
    "import glob\n",
    "import sys\n",
    "import os.path\n",
    "import copy\n",
    "import time\n",
    "import ast\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "from twython import Twython, TwythonRateLimitError, TwythonError\n",
    "import logging\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ProcessPoolExecutor as PoolExecutor\n",
    "import multiprocessing\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy\n",
    "engine9 = create_engine(\"mysql+pymysql://marksproject:IJKDEJFRknnkfr!!78278w2kjde@145.100.59.121/publicsphere?charset=utf8mb4\")\n",
    "con = engine9.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:08:37.317870Z",
     "start_time": "2019-12-23T22:08:37.244091Z"
    }
   },
   "outputs": [],
   "source": [
    "def del_scraper(screen_name,password,main_dir,TweetScraper_path,saving_path,files_path):\n",
    "    try:\n",
    "        #Moving files to dedicated folder\n",
    "        if os.path.isdir(TweetScraper_path) is True:\n",
    "            print(\"{} Moving any files to dedicated folder {}.\".format(screen_name,saving_path))\n",
    "            for tweet_file in os.listdir(files_path):\n",
    "                if tweet_file.startswith(\"{}\".format(screen_name)):\n",
    "                    shutil.move(tweet_file, saving_path)\n",
    "            print(\"{} FILES MOVED TO FOLDER {}.\".format(screen_name,saving_path,\"/\",screen_name))\n",
    "            sudo = shlex.split(\"sudo -u\")\n",
    "            del_folder = shlex.split(\"rm -rf TweetScraper\")\n",
    "            # Deleting old TweepScraper files\n",
    "            print(\"ATTN: DELETING TWEETSCRAPER FILES, PRESS ENTER TO CONTINUE.\")\n",
    "            print(\"Move tweet folder NOW if you do not want the files to be deleted. DELETE MANUALLY IF MOVING FILES.\")\n",
    "            yes = {'yes','y', 'ye', ''}\n",
    "            no = {'no','n'}\n",
    "            choice = input(\"Type 'YES' to delete TweetScraper files (Note: check if tweet files are saved before).\").lower()\n",
    "            if choice in yes:\n",
    "                subprocess.Popen(\"sudo\",shell=True,stdout=subprocess.PIPE)\n",
    "                subprocess.Popen(password,shell=True,stdout=subprocess.PIPE)\n",
    "                subprocess.Popen(del_folder,shell=True,stdout=subprocess.PIPE)\n",
    "                shutil.rmtree(files_path, ignore_errors=True)\n",
    "            elif choice in no:\n",
    "                return False\n",
    "            else:\n",
    "                sys.stdout.write(\"Please respond with 'yes' or 'no'\")\n",
    "            \n",
    "        elif os.path.isdir(TweetScraper_path) is False:\n",
    "            print(\"No TweetScraper folder found.\")\n",
    "    except SyntaxError:\n",
    "        pass\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:08:37.354433Z",
     "start_time": "2019-12-23T22:08:37.330964Z"
    }
   },
   "outputs": [],
   "source": [
    "def scraper(screen_name,password,main_dir,TweetScraper_path,saving_path,files_path,since,until):\n",
    "    try:\n",
    "        sudo = shlex.split(\"sudo -u\")\n",
    "        del_scraper(screen_name,password,main_dir,TweetScraper_path,saving_path,files_path)\n",
    "        os.chdir(main_dir)\n",
    "        print(\"Downloading TweepScraper in {}.\".format(TweetScraper_path))\n",
    "        os.system(\"git clone https://github.com/jonbakerfish/TweetScraper.git\")\n",
    "        print(\"Download complete. Checking TweetScraper.\")\n",
    "        os.chdir(\"TweetScraper/\")\n",
    "        os.system(\"scrapy list\")\n",
    "        command1 = shlex.split('scrapy crawl TweetScraper -a query=\"from:{}\"'.format(screen_name))\n",
    "        command2 = shlex.split('scrapy crawl TweetScraper -a query=\"to:{} since:{} --until:{}\"'.format(screen_name,since,until))\n",
    "        if (since == None) and (until == None):\n",
    "            print(\"This command will be run in the terminal: \",command1)\n",
    "            print(\"{} Getting tweets from TweetScraper. This may take some time so please wait...\".format(screen_name))\n",
    "            subprocess.Popen(sudo,universal_newlines=True,shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "            subprocess.Popen(password,universal_newlines=True,shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "            cmd = subprocess.Popen(command1,universal_newlines=True,shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "            print(subprocess.check_output(command1))\n",
    "            print(\"Command sent. Waiting till download is finished...\")\n",
    "            cmd.communicate()\n",
    "        elif (since != None) and (until != None):\n",
    "            print(\"This command will be run in the terminal: \",command2)\n",
    "            print(\"{} Getting replies from TweetScraper. This may take some time so please wait...\".format(screen_name,since,until))\n",
    "            subprocess.Popen(sudo,universal_newlines=True,shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "            subprocess.Popen(password,universal_newlines=True,shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "            cmd = subprocess.Popen(command2,universal_newlines=True,shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "            print(subprocess.check_output(command1))\n",
    "            print(\"Command sent. Waiting till download is finished...\")\n",
    "            cmd.communicate()\n",
    "        print(\"Scraping complete for {}.\".format(screen_name))\n",
    "    except SyntaxError:\n",
    "        pass\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:08:37.378252Z",
     "start_time": "2019-12-23T22:08:37.365131Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_saving_path(screen_name,saving_path):\n",
    "    if not os.path.exists(saving_path):\n",
    "        print(\"Creating a dedicated file for {} at .\".format(screen_name,saving_path))\n",
    "        os.makedirs(saving_path)\n",
    "    elif os.path.exists(saving_path):\n",
    "        if os.listdir(saving_path):\n",
    "            print(\"There seems to already be a dedicated folder for {} at {}. Please move files and empty folder.\".format(screen_name,saving_path))\n",
    "            yes = {'yes','y', 'ye', ''}\n",
    "            no = {'no','n'}\n",
    "            choice = input(\"Type 'YES' to delete existing folder. Note: Make sure tweet files are saved somewhere else.\").lower()\n",
    "            if choice in yes:\n",
    "                del_tweets = shlex.split(\"rm -rf tweets\")\n",
    "                subprocess.Popen(del_tweets,shell=True,stdout=subprocess.PIPE)\n",
    "                del_replies = shlex.split(\"rm -rf replies\")\n",
    "                subprocess.Popen(del_replies,shell=True,stdout=subprocess.PIPE)\n",
    "            elif choice in no:\n",
    "                sys.exit(\"Script will bnow be terminated.\")\n",
    "        elif not os.listdir(saving_path):\n",
    "            pass\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:08:37.388571Z",
     "start_time": "2019-12-23T22:08:37.382093Z"
    }
   },
   "outputs": [],
   "source": [
    "def open_files(screen_name,ids,path):\n",
    "    print(\"{} getting IDs from TweetScrapper.\".format(screen_name))\n",
    "    for file in os.listdir(path):\n",
    "        ids.append(int(file))\n",
    "        print(\"{} {} tweets gotten from TweetScrapper.\".format(screen_name,len(ids)))\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:08:37.401137Z",
     "start_time": "2019-12-23T22:08:37.392320Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sql(screen_name):\n",
    "    data = pd.read_sql_query(\"SELECT id_str, created_at FROM Twitter_Posts WHERE twitter_name = '\"+screen_name+\"'\" , con=con)\n",
    "    df_tweet =  pd.DataFrame(data,columns=['id_str','created_at'])\n",
    "    df_tweet['created_at'] = pd.to_datetime(df_tweet['created_at']).dt.date\n",
    "    created_at = df_tweet['created_at'].tolist()\n",
    "    since = min(created_at)\n",
    "    until = max(created_at)\n",
    "    alltweets = df_tweet['id_str'].tolist()\n",
    "    print(\"{} getting {} ids from SQL as alltweets.\".format(screen_name,len(alltweets)))\n",
    "    print(\"Since: \",since)\n",
    "    print(\"Until: \",until)\n",
    "    return alltweets,since,until"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:08:37.413842Z",
     "start_time": "2019-12-23T22:08:37.406764Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_ids(screen_name,alltweets,ids,len_ids):\n",
    "    for d in alltweets:\n",
    "        d['id']=int(d['id'])\n",
    "        for _id in ids:\n",
    "            if _id == d['id']:\n",
    "                ids.remove(_id)\n",
    "                new_len_ids = len(ids)\n",
    "                removed = len_ids - new_len_ids\n",
    "                print(\"{} IDs removed: {}\".format(screen_name,removed))\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:08:37.459167Z",
     "start_time": "2019-12-23T22:08:37.443892Z"
    }
   },
   "outputs": [],
   "source": [
    "#getting replies\n",
    "def filter_replies(screen_name,alltweets,replytweets,final_replytweets,counter,big_counter):\n",
    "    counter += 1\n",
    "    big_counter += 1\n",
    "    left = len(replytweets) - len(final_replytweets)\n",
    "    for tweet in alltweets:\n",
    "        for reply in replytweets:\n",
    "            for key,value in reply.items():\n",
    "                if 'in_reply_to_status_id' in key:\n",
    "                    if (value is not None) and (value == tweet):\n",
    "                        final_replytweets.append(reply)\n",
    "                        print(\"{} {} directed comment tweets downloaded so far.\".format(screen_name,len(final_replytweets)))\n",
    "                        if counter == 100:\n",
    "                            print(\"{} directed tweets left to finish: {}\".format(screen_name,left))\n",
    "                        if counter == 10:\n",
    "                            print(\"{} counter reached {}, saving tweets.\".format(screen_name,big_counter))\n",
    "                            with open(screen_name+'_final_replytweets.json', 'w') as f:\n",
    "                                json.dump(final_replytweets, f)\n",
    "                            print(\"{} {} TWEETS SAVED.\".format(screen_name,len(final_replytweets)))\n",
    "                            counter = 0\n",
    "    return final_replytweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:08:37.483151Z",
     "start_time": "2019-12-23T22:08:37.470894Z"
    }
   },
   "outputs": [],
   "source": [
    "#add administrative columns\n",
    "def add_cols(x):\n",
    "    add_cols = []\n",
    "    for i in x:\n",
    "        if 'extended_entities' in i:\n",
    "            if i['extended_entities']['media'][0]['type'] == 'video':\n",
    "                add_cols.append([i['extended_entities']['media'][0]['type'],i['user']['id'],i['id']])\n",
    "            elif i['extended_entities']['media'][0]['type'] != 'video':\n",
    "                add_cols.append([\"not video\",i['user']['id'],i['id']])\n",
    "        elif 'extended_entities' not in i:\n",
    "            add_cols.append([\"not video\",i['user']['id'],i['id']])\n",
    "\n",
    "    return add_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:08:37.499972Z",
     "start_time": "2019-12-23T22:08:37.489581Z"
    }
   },
   "outputs": [],
   "source": [
    "#flatten alltweets\n",
    "def flatten(x):\n",
    "    d = copy.deepcopy(x)\n",
    "    for key in list(d):\n",
    "        if isinstance(d[key], list):\n",
    "            value = d.pop(key)\n",
    "            for i, v in enumerate(value):\n",
    "                d.update(flatten({'{}_{}'.format(key, i): v}))\n",
    "        elif isinstance(d[key], dict):\n",
    "            d[key] = str(d[key])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:08:37.517540Z",
     "start_time": "2019-12-23T22:08:37.506055Z"
    }
   },
   "outputs": [],
   "source": [
    "def genre(df_tweet):\n",
    "    op_news = [\"AC360\",\"TuckerCarlson\",\"hardball\"]\n",
    "    news = [\"CBSEveningNews\",\"11thHour\",\"NewsHour\",\"ABCWorldNews\",\"NightLine\",\"FaceTheNation\",\"60Minutes\",\"NBCNews\",\"MeetThePress\",\"NOS\",\"nosop3\",\"RTLnieuws\",\"Nieuwsuur\"]\n",
    "    parody = [\"SouthPark\",\"nbcsnl\",\"TheOnion\"]\n",
    "    if df_tweet[\"twitter_name\"] in op_news:\n",
    "        return \"Opinionated news\"\n",
    "    if df_tweet[\"twitter_name\"] in news:\n",
    "        return \"News\"\n",
    "    if df_tweet[\"twitter_name\"] in parody:\n",
    "        return \"Parody\"\n",
    "    else:\n",
    "        return \"News satire\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:08:37.539291Z",
     "start_time": "2019-12-23T22:08:37.522427Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_df(screen_name,alltweets):\n",
    "    \n",
    "    if len(alltweets) > 0:\n",
    "        #write DataFrame\n",
    "        print(\"{} Flatten df.\".format(screen_name))\n",
    "        df_tweet_flat = pd.DataFrame([flatten(tweet) for tweet in alltweets])\n",
    "        df_tweet_flat = df_tweet_flat.drop_duplicates(subset=\"id\")\n",
    "\n",
    "        #add_cols\n",
    "        print(\"{} Adding columns.\".format(screen_name))\n",
    "        add_tweet_cols = add_cols(alltweets)\n",
    "        df_add_tweet_cols = pd.DataFrame(add_tweet_cols, columns = [\"video\",\"user_id\",\"id\"])\n",
    "        df_tweet = pd.merge(df_add_tweet_cols.set_index(\"id\"),df_tweet_flat.set_index(\"id\"), right_index=True, left_index=True).reset_index()\n",
    "        print(\"{} Fixing date.\".format(screen_name))\n",
    "        df_tweet['created_at'] = pd.to_datetime(df_tweet['created_at']).dt.date\n",
    "        df_tweet.insert(0, \"platform\", \"Twitter\")\n",
    "        df_tweet.insert(1, \"twitter_name\", screen_name)\n",
    "        df_tweet[\"rowNumber\"] = np.arange(len(df_tweet))\n",
    "        print(\"{} Adding genre.\".format(screen_name))\n",
    "        df_tweet[\"genre\"] = df_tweet.apply(genre, axis=1)\n",
    "        cols = list(df_tweet)\n",
    "        print(\"{} Adding rowNumber.\".format(screen_name))\n",
    "        cols.insert(0, cols.pop(cols.index(\"rowNumber\")))\n",
    "        cols.insert(3, cols.pop(cols.index(\"genre\")))\n",
    "        df_tweet = df_tweet.loc[:, cols]\n",
    "        print(\"{} Clearing Duplicates.\".format(screen_name))\n",
    "        df_tweet = df_tweet.drop_duplicates(subset=\"id\")\n",
    "        #df_tweet = df_tweet.loc[df_tweet.astype(str).drop_duplicates().index]\n",
    "    \n",
    "    elif len(alltweets) == 0:\n",
    "        print(\"No tweets in list.\")\n",
    "        df_tweet = pd.DataFrame()\n",
    "        pass\n",
    "    return df_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:08:37.782944Z",
     "start_time": "2019-12-23T22:08:37.548536Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_all(screen_name):\n",
    "    \n",
    "    print(\"To begin, please enter the admin password for this machine.\")\n",
    "    password = getpass.getpass()\n",
    "    \n",
    "    ###### Path where scraped Tweetscraper and tweet files will be saved ###### \n",
    "    \n",
    "    yes = {'yes','y', 'ye', ''}\n",
    "    no = {'no','n'}\n",
    "\n",
    "    choice = input(\"Would you like to save files in a new location?\").lower()\n",
    "    if choice in yes:\n",
    "        #Path to choosen directory\n",
    "        main_dir = input('Enter path to main directory where TweetScraper will be downloaded, e.g. /Users/home')\n",
    "        saving_dir = input('Enter path to download directory where tweets and replies will be downloaded, e.g. /Users/home')\n",
    "        \n",
    "    elif choice in no:\n",
    "        #Path to current directory\n",
    "        main_dir = os.getcwd()\n",
    "        saving_dir = os.getcwd()\n",
    "        print(\"Saving files for {} in current directory at {}\".format(screen_name,main_dir))\n",
    "    \n",
    "    else:\n",
    "        sys.stdout.write(\"Please respond with 'yes' or 'no'\")\n",
    "    \n",
    "    #Path to TweetScraper\n",
    "    print(\"This is the directory where TweetScraper will be saved: {}\".format(main_dir))\n",
    "    files_path = '{}/TweetScraper/'.format(main_dir)\n",
    "    TweetScraper_path = '{}/TweetScraper/Data/tweet/'.format(main_dir)\n",
    "    \n",
    "    #Path where ONLY tweet files will be saved\n",
    "    saving_path_tweets = '{}/{}/tweets/'.format(saving_dir,screen_name)\n",
    "    print(\"This is the directory where ONLY tweets will be saved: {}\".format(saving_path_tweets))\n",
    "    create_saving_path(screen_name,saving_path_tweets)\n",
    "\n",
    "    #Path where ONLY reply files will be saved\n",
    "    saving_path_replies = '{}/{}/replies/'.format(saving_dir,screen_name)\n",
    "    print(\"This is the directory where ONLY replies will be saved: {}\".format(saving_path_replies))\n",
    "    create_saving_path(screen_name,saving_path_replies)\n",
    "    \n",
    "    ############################################################################################\n",
    "    ######################################### TWEETS ###########################################\n",
    "    ############################################################################################\n",
    "    if os.path.isdir(TweetScraper_path) is False:\n",
    "        #scrape tweets from Tweetscraper\n",
    "        print(\"TweepScraper not downloaded yet.\")\n",
    "        scraper(screen_name,password,main_dir,TweetScraper_path,saving_path_tweets,files_path,None,None)\n",
    "    \n",
    "    elif os.path.isdir(TweetScraper_path) is True:\n",
    "        print(\"{} Resuming from TweetScraper folder for tweets.\".format(screen_name))\n",
    "    \n",
    "    if os.path.isfile('{}_final_replytweets.json'.format(screen_name)) is False:\n",
    "        print(\"Creating new final_replytweets file for {}.\".format(screen_name))\n",
    "        final_replytweets = []\n",
    "        \n",
    "        if os.path.isfile('{}_replytweets.json'.format(screen_name)) is False:\n",
    "            print(\"Creating new replytweets file for {}.\".format(screen_name))\n",
    "            replytweets = []\n",
    "\n",
    "            if os.path.isfile('{}_df_tweet.json'.format(screen_name)) is False:\n",
    "                print(\"Creating new df_tweet for {}.\".format(screen_name))\n",
    "\n",
    "                if os.path.isfile('{}_alltweets.json'.format(screen_name)) is False:\n",
    "                    print(\"Creating new alltweets file for {}.\".format(screen_name))\n",
    "                    alltweets = []\n",
    "\n",
    "                elif os.path.isfile('{}_alltweets.json'.format(screen_name)) is True:\n",
    "                    print(\"Resuming from old alltweets file for {}.\".format(screen_name))\n",
    "                    with open('{}_alltweets.json'.format(screen_name), 'r') as f:\n",
    "                        alltweets = json.load(f)\n",
    "\n",
    "                #Getting counter\n",
    "                print(\"{} getting counter.\".format(screen_name))\n",
    "                counter_tweet_ids = []\n",
    "                counter_tweet_ids = open_files(screen_name,counter_tweet_ids,TweetScraper_path)\n",
    "                print(\"{} Clearing duplicate tweet IDs.\".format(screen_name))\n",
    "                counter_tweet_ids = list(set(counter_tweet_ids))\n",
    "                len_counter_tweet_ids = len(counter_tweet_ids)\n",
    "                if os.path.isfile('{}_tweet_ids.json'.format(screen_name)) is False:\n",
    "                    tweet_ids = counter_tweet_ids\n",
    "                    print(\"{} Saving tweet IDs list of length: {}\".format(screen_name,len(tweet_ids)))\n",
    "                    with open('{}_tweet_ids.json'.format(screen_name), 'w') as f:\n",
    "                        json.dump(tweet_ids, f)\n",
    "\n",
    "                if len(alltweets) != len_counter_tweet_ids:\n",
    "\n",
    "                    #Get tweet IDs\n",
    "                    if os.path.isfile('{}_tweet_ids.json'.format(screen_name)) is True:\n",
    "                        print(\"Resuming from old tweet IDs file for {}.\".format(screen_name))\n",
    "                        with open('{}_tweet_ids.json'.format(screen_name), 'r') as f:\n",
    "                            tweet_ids = json.load(f)\n",
    "                        print(\"{} Clearing duplicate tweet IDs.\".format(screen_name))\n",
    "                        tweet_ids = list(set(tweet_ids))\n",
    "                        print(\"{} Saving backup tweet IDs.\".format(screen_name))\n",
    "                        with open('{}_tweet_ids_backup.json'.format(screen_name), 'w') as f:\n",
    "                            json.dump(tweet_ids, f)\n",
    "                        print(\"{} Lenght of tweet IDs list: {}\".format(screen_name,len(tweet_ids)))\n",
    "                    elif os.path.isfile('{}_tweet_ids.json'.format(screen_name)) is False:\n",
    "                        print(\"Creating new tweet ids file for {}.\".format(screen_name))\n",
    "                        print(\"{} Length of new tweet IDs list: {}.\".format(screen_name,len(tweet_ids)))\n",
    "\n",
    "                    #filter used ids from ids list\n",
    "                    print(\"{} Getting tweet IDs.\".format(screen_name))\n",
    "                    tweet_ids = get_ids(screen_name,alltweets,tweet_ids,len_counter_tweet_ids)\n",
    "                    tweet_ids = list(set(tweet_ids))\n",
    "                    print(\"{} Savings tweet IDs.\".format(screen_name))\n",
    "                    with open('{}_tweet_ids.json'.format(screen_name), 'w') as f:\n",
    "                        json.dump(tweet_ids, f)\n",
    "                    print(\"{} Saving backup tweet IDs.\".format(screen_name))\n",
    "                    with open('{}_tweet_ids_backup.json'.format(screen_name), 'w') as f:\n",
    "                        json.dump(tweet_ids, f)\n",
    "                    print(\"{} Ajusted lenght of tweet IDs list: {}\".format(screen_name,len(tweet_ids)))\n",
    "\n",
    "                    ######################## GET TWEETS ########################\n",
    "\n",
    "                    #Twitter API tokens\n",
    "                    access_key = \"1196733070210215937-kOtTpMiS1BFu7FYzEgFbkzmQKDyvGC\"\n",
    "                    access_secret = \"mLot2xban1C0774E2N9fJAzPJWkoRT31Iik4MHQtbZMcQ\"\n",
    "                    consumer_key = \"gMwo76vm414OWvYnkiNqyv6LE\"\n",
    "                    consumer_secret = \"3wMHuySlzbHrSvHXG6LFZ9M6vpHXJVVJTNx92QUTU2UHcN13hh\"\n",
    "                    #authorize twitter, initialize tweepy\n",
    "                    api = Twython(consumer_key, consumer_secret, access_key, access_secret)\n",
    "\n",
    "                    counter = 0\n",
    "                    big_counter = 0\n",
    "                    tweet_ids_chunks = [tweet_ids[i:i+100] for i in range(0, len(tweet_ids), 100)]\n",
    "                    for tweet_id in tweet_ids_chunks:\n",
    "                        try:\n",
    "                            new_tweets = api.lookup_status(id=tweet_id,include_entities=True)\n",
    "                            for tweet in new_tweets:\n",
    "                                alltweets.append(tweet)\n",
    "                            print(\"{} {} tweets downloaded so far.\".format(screen_name,len(alltweets)))\n",
    "                            counter += 1\n",
    "                            big_counter += 1\n",
    "                            left = len_counter_tweet_ids - len(alltweets)\n",
    "                            if big_counter == 10:\n",
    "                                print(\"{} counter reached {}, saving tweets.\".format(screen_name,big_counter))\n",
    "                                print(\"{} {} tweet IDs left to finish\".format(screen_name,left))\n",
    "                                with open('{}_alltweets.json'.format(screen_name), 'w') as f:\n",
    "                                    json.dump(alltweets, f)\n",
    "                                with open('{}_alltweets_backup.json'.format(screen_name), 'w') as f:\n",
    "                                    json.dump(alltweets, f)\n",
    "                                print(\"{} {} TWEETS SAVED.\".format(screen_name,len(alltweets)))\n",
    "                            if counter == 1000:\n",
    "                                print(\"{} counter reached {}, saving tweets.\".format(screen_name,big_counter))\n",
    "                                print(\"{} {} tweet IDs left to finish\".format(screen_name,left))\n",
    "                                with open('{}_alltweets.json'.format(screen_name), 'w') as f:\n",
    "                                    json.dump(alltweets, f)\n",
    "                                with open('{}_alltweets_backup.json'.format(screen_name), 'w') as f:\n",
    "                                    json.dump(alltweets, f)\n",
    "                                print(\"{} {} TWEETS SAVED.\".format(screen_name,len(alltweets)))\n",
    "                                counter = 0\n",
    "                        except TwythonRateLimitError as error:\n",
    "                            with open('{}_alltweets.json'.format(screen_name), 'w') as f:\n",
    "                                json.dump(alltweets, f)\n",
    "                            with open('{}_alltweets_backup.json'.format(screen_name), 'w') as f:\n",
    "                                json.dump(alltweets, f)\n",
    "                            print(\"{} Error {} at tweet id: {}\".format(screen_name,error.error_code, alltweets[-1]['id']))\n",
    "                            remainder = abs(float(api.get_lastfunction_header(header='x-rate-limit-reset')) - time.time())\n",
    "                            del api\n",
    "                            print(\"{} Resuming in {} seconds\".format(screen_name,remainder))\n",
    "                            time.sleep(remainder)\n",
    "                            api = Twython(consumer_key, consumer_secret, access_key, access_secret)\n",
    "                            logging.basicConfig(level=logging.ERROR)\n",
    "                            logging.error('This error occured: {}'.format(error))\n",
    "                        except TwythonError as error:\n",
    "                            print(\"{} Error {} at tweet id: {}\".format(screen_name,error.error_code, alltweets[-1]['id']))\n",
    "                            for i in tweet_id:\n",
    "                                tweet_ids.remove(i)\n",
    "                                for file in os.listdir(path):\n",
    "                                    if i == int(file):\n",
    "                                        os.remove(file)\n",
    "                            with open('{}_tweet_ids.json'.format(screen_name), 'w') as f:\n",
    "                                json.dump(tweet_ids, f)\n",
    "                            with open('{}_tweet_ids_backup.json'.format(screen_name), 'w') as f:\n",
    "                                json.dump(tweet_ids, f)\n",
    "                            with open('{}_alltweets_backup.json'.format(screen_name), 'w') as f:\n",
    "                                json.dump(alltweets, f)\n",
    "                            pass\n",
    "\n",
    "                    #END OF ALLTWEETS COLLECTION\n",
    "                    print(\"{} finished downloading {} alltweets. TWEETS COLLECTED. Saving files.\".format(screen_name, len(alltweets)))\n",
    "                    with open('{}_alltweets.json'.format(screen_name), 'w') as f:\n",
    "                        json.dump(alltweets, f)\n",
    "                    with open('{}_alltweets_backup.json'.format(screen_name), 'w') as f:\n",
    "                        json.dump(alltweets, f)\n",
    "\n",
    "                elif len(alltweets) == len_counter_tweet_ids:\n",
    "                    print(\"{} already collected all {} alltweets.\".format(screen_name,len(alltweets)))\n",
    "\n",
    "                #clean output\n",
    "    #             print(\"{} Clearing alltweets of lenght: {}\".format(len(alltweets)))\n",
    "    #             alltweets = [i for n, i in enumerate(alltweets) if i not in alltweets[n + 1:]]\n",
    "\n",
    "                #write DataFrame\n",
    "                print(\"{} Writing df_tweet.\".format(screen_name))\n",
    "                df_tweet = write_df(screen_name,alltweets)\n",
    "                print(\"{} Saving df_tweet of length: {}.\".format(screen_name,len(df_tweet)))\n",
    "                df_tweet.to_json(screen_name+'_df_tweet.json')\n",
    "                df_tweet.to_csv(screen_name+'_df_tweet.csv',index=False)\n",
    "                print(\"{} JSON and CSV files have been saved with {} tweets. DONE!\".format(screen_name,len(df_tweet)))\n",
    "\n",
    "            elif os.path.isfile('{}_df_tweet.json'.format(screen_name)) is True:\n",
    "                print(\"{} already collected all df_tweet.\".format(screen_name))\n",
    "                df_tweet = pd.read_csv('{}_df_tweet.csv'.format(screen_name))\n",
    "                with open('{}_alltweets.json'.format(screen_name), 'r') as f:\n",
    "                    alltweets = json.load(f)\n",
    "                print(\"{} Length to alltweets: {}.\".format(screen_name,len(alltweets)))\n",
    "                print(\"{} Length to df_tweet: {}.\".format(screen_name,len(df_tweet)))\n",
    "            \n",
    "            #Deleting TweetScraper tweet files\n",
    "            del_scraper(screen_name,password,main_dir,TweetScraper_path,saving_path_tweets,files_path)\n",
    "            \n",
    "            ############################################################################################\n",
    "            ######################################### REPLIES ##########################################\n",
    "            ############################################################################################\n",
    "\n",
    "            #get alltweets from SQL and scrape twitter\n",
    "    #         alltweets,since,until = get_sql(screen_name)\n",
    "            # Creating since and until variables\n",
    "            df_tweet['created_at'] = pd.to_datetime(df_tweet['created_at']).dt.date\n",
    "            created_at = df_tweet['created_at'].tolist()\n",
    "    #         created_at = list(alltweets[0]['created_at'])\n",
    "            since = min(created_at)\n",
    "            until = max(created_at)\n",
    "            print(\"{} Alltweets since: {}, until: {}.\".format(screen_name,since,until))\n",
    "\n",
    "            if os.path.isdir(TweetScraper_path) is False:\n",
    "                #scrape tweets from Tweetscraper\n",
    "                print(\"TweepScraper not downloaded yet.\")\n",
    "                scraper(screen_name,password,main_dir,TweetScraper_path,saving_path_replies,files_path,since,until)\n",
    "\n",
    "            elif os.path.isdir(TweetScraper_path) is True:\n",
    "                print(\"{} Resuming from TweetScraper folder for tweets.\".format(screen_name))\n",
    "\n",
    "            #Getting counter\n",
    "            print(\"{} getting counter.\".format(screen_name))\n",
    "            counter_reply_ids = []\n",
    "            counter_reply_ids = open_files(screen_name,counter_reply_ids,TweetScraper_path)\n",
    "            print(\"{} Clearing duplicate reply IDs.\".format(screen_name))\n",
    "            counter_reply_ids = list(set(counter_reply_ids))\n",
    "            len_counter_reply_ids = len(counter_reply_ids)\n",
    "            if os.path.isfile('{}_reply_ids.json'.format(screen_name)) is False:\n",
    "                reply_ids = counter_reply_ids\n",
    "                print(\"{} Saving reply IDs list of length: {}\".format(screen_name,len(reply_ids)))\n",
    "                with open('{}_reply_ids.json'.format(screen_name), 'w') as f:\n",
    "                    json.dump(reply_ids, f)\n",
    "                print(\"{} Saving reply IDs backups\".format(screen_name))\n",
    "                with open('{}_reply_ids_backups.json'.format(screen_name), 'w') as f:\n",
    "                    json.dump(reply_ids, f)\n",
    "\n",
    "        ######################## GET REPLIES ########################\n",
    "\n",
    "        elif os.path.isfile('{}_replytweets.json'.format(screen_name)) is True:\n",
    "            print(\"Resuming from old replytweets file for {}.\".format(screen_name))\n",
    "            with open('{}_replytweets.json'.format(screen_name), 'r') as f:\n",
    "                replytweets = json.load(f)\n",
    "            print(\"{} Clearing duplicate replytweets.\".format(screen_name))\n",
    "            replytweets = [i for n, i in enumerate(replytweets) if i not in replytweets[n + 1:]]\n",
    "            print(\"{} Saving backup replytweets.\".format(screen_name))\n",
    "            with open('{}_replytweets_backup.json'.format(screen_name), 'w') as f:\n",
    "                json.dump(replytweets, f)\n",
    "            print(\"{} Lenght of replytweets list: {}\".format(screen_name,len(replytweets)))\n",
    "\n",
    "        if len(replytweets) != len_counter_reply_ids:\n",
    "\n",
    "            #open files and get ids\n",
    "            if os.path.isfile('{}_reply_ids.json'.format(screen_name)) is True:\n",
    "                print(\"Resuming from old reply IDs file for {}.\".format(screen_name))\n",
    "                with open('{}_reply_ids.json'.format(screen_name), 'r') as f:\n",
    "                    reply_ids = json.load(f)\n",
    "                print(\"{} Clearing duplicate reply IDs.\".format(screen_name))\n",
    "                reply_ids = list(set(reply_ids))\n",
    "                print(\"{} Saving backup reply IDs.\".format(screen_name))\n",
    "                with open('{}_reply_ids_backup.json'.format(screen_name), 'w') as f:\n",
    "                    json.dump(reply_ids, f)\n",
    "                print(\"{} Lenght of IDs list: {}\".format(screen_name,len(reply_ids)))\n",
    "            elif os.path.isfile('{}_reply_ids.json'.format(screen_name)) is False:\n",
    "                print(\"Creating new ids file for {}.\".format(screen_name))\n",
    "                print(\"{} Length of new reply IDs list: {}.\".format(screen_name,len(reply_ids)))\n",
    "\n",
    "            #filter used ids from ids list\n",
    "            print(\"{} Getting reply IDs.\".format(screen_name))\n",
    "            reply_ids = get_ids(screen_name,replytweets,reply_ids,len_counter_reply_ids)\n",
    "            reply_ids = list(set(reply_ids))\n",
    "            print(\"{} Savings reply IDs.\".format(screen_name))\n",
    "            with open('{}_reply_ids.json'.format(screen_name), 'w') as f:\n",
    "                json.dump(reply_ids, f)\n",
    "            print(\"{} Saving backup IDs.\".format(screen_name))\n",
    "            with open('{}_reply_ids_backup.json'.format(screen_name), 'w') as f:\n",
    "                json.dump(reply_ids, f)\n",
    "            print(\"{} Ajusted lenght of reply IDs list: {}\".format(screen_name,len(reply_ids)))\n",
    "\n",
    "            #GET REPLIES\n",
    "\n",
    "            #Twitter API 3 tokens\n",
    "            access_key = \"1196733070210215937-kOtTpMiS1BFu7FYzEgFbkzmQKDyvGC\"\n",
    "            access_secret = \"mLot2xban1C0774E2N9fJAzPJWkoRT31Iik4MHQtbZMcQ\"\n",
    "            consumer_key = \"gMwo76vm414OWvYnkiNqyv6LE\"\n",
    "            consumer_secret = \"3wMHuySlzbHrSvHXG6LFZ9M6vpHXJVVJTNx92QUTU2UHcN13hh\"\n",
    "            #authorize twitter, initialize tweepy\n",
    "            api = Twython(consumer_key, consumer_secret, access_key, access_secret)\n",
    "\n",
    "            counter = 0\n",
    "            big_counter = 0\n",
    "            reply_ids_chunks = [reply_ids[i:i+100] for i in range(0, len(reply_ids), 100)]\n",
    "            for reply_id in reply_ids_chunks:\n",
    "                try:\n",
    "                    new_replytweets = api.lookup_status(id=reply_id,include_entities=True)\n",
    "                    for reply in new_replytweets:\n",
    "                        replytweets.append(reply)\n",
    "                    print(\"{} {} raw comment tweets downloaded so far.\".format(screen_name,len(replytweets)))\n",
    "                    counter += 1\n",
    "                    big_counter += 1\n",
    "                    left = len_counter_reply_ids - len(replytweets)\n",
    "                    if big_counter == 10:\n",
    "                        print(\"{} counter reached {}, saving tweets.\".format(screen_name,big_counter))\n",
    "                        print(\"{} {} reply IDs left to finish\".format(screen_name,left))\n",
    "                        with open('{}_replytweets.json'.format(screen_name), 'w') as f:\n",
    "                            json.dump(replytweets, f)\n",
    "                        with open('{}_replytweets_backup.json'.format(screen_name), 'w') as f:\n",
    "                            json.dump(replytweets, f)\n",
    "                        print(\"{} {} REPLY TWEETS SAVED.\".format(screen_name,len(replytweets)))\n",
    "                    if counter == 1000:\n",
    "                        print(\"{} counter reached {}, saving tweets.\".format(screen_name,big_counter))\n",
    "                        print(\"{} {} reply IDs left to finish\".format(screen_name,left))\n",
    "                        with open('{}_replytweets.json'.format(screen_name), 'w') as f:\n",
    "                            json.dump(replytweets, f)\n",
    "                        with open('{}_replytweets_backup.json'.format(screen_name), 'w') as f:\n",
    "                            json.dump(replytweets, f)\n",
    "                        print(\"{} {} REPLY TWEETS SAVED.\".format(screen_name,len(replytweets)))\n",
    "                        counter = 0\n",
    "                except TwythonRateLimitError as error:\n",
    "                    print(\"{} Error {} at reply id: {}\".format(screen_name,error.error_code, replytweets[-1]['id']))\n",
    "                    remainder = abs(float(api.get_lastfunction_header(header='x-rate-limit-reset')) - time.time())\n",
    "                    del api\n",
    "                    print(\"{} Resuming in {} seconds\".format(screen_name,remainder))\n",
    "                    time.sleep(remainder)\n",
    "                    api = Twython(consumer_key, consumer_secret, access_key, access_secret)\n",
    "                    with open('{}_replytweets.json'.format(screen_name), 'w') as f:\n",
    "                        json.dump(replytweets, f)\n",
    "                    with open('{}_replytweets_backup.json'.format(screen_name), 'w') as f:\n",
    "                        json.dump(replytweets, f)\n",
    "                    logging.basicConfig(level=logging.ERROR)\n",
    "                    logging.error('This error occured: {}'.format(error))\n",
    "                except TwythonError as error:\n",
    "                    print(\"{} Error {} at reply id: {}\".format(screen_name,error.error_code, replytweets[-1]['id']))\n",
    "                    for i in reply_id:\n",
    "                        reply_ids.remove(i)\n",
    "                        for file in os.listdir(path):\n",
    "                            if i == int(file):\n",
    "                                os.remove(file)\n",
    "                    with open('{}_reply_ids.json'.format(screen_name), 'w') as f:\n",
    "                        json.dump(reply_ids, f)\n",
    "                    with open('{}_reply_ids_backup.json'.format(screen_name), 'w') as f:\n",
    "                        json.dump(reply_ids, f)\n",
    "                    with open('{}_replytweets_backup.json'.format(screen_name), 'w') as f:\n",
    "                        json.dump(replytweets, f)\n",
    "                    pass\n",
    "\n",
    "            #END OF REPLYTWEETS COLLECTION\n",
    "            print(\"{} finished downloading {} replytweets. REPLY TWEETS COLLECTED. Saving files.\".format(screen_name, len(replytweets)))\n",
    "            with open('{}_replytweets.json'.format(screen_name), 'w') as f:\n",
    "                json.dump(replytweets, f)\n",
    "            with open('{}_replytweets_backup.json'.format(screen_name), 'w') as f:\n",
    "                json.dump(replytweets, f)\n",
    "\n",
    "        elif len(replytweets) == len_counter_reply_ids:\n",
    "            print(\"{} already collected all {} replytweets.\".format(screen_name,len(replytweets)))\n",
    "\n",
    "    elif (os.path.isfile('{}_final_replytweets.json'.format(screen_name)) is True) and (os.path.isfile('{}_replytweets.json'.format(screen_name)) is True) :\n",
    "        print(\"Resuming from old final_replytweets file for {}.\".format(screen_name))\n",
    "        with open('{}_final_replytweets.json'.format(screen_name), 'r') as f:\n",
    "            final_replytweets = json.load(f)\n",
    "        print(\"Resuming from old replytweets file for {}.\".format(screen_name))\n",
    "        with open('{}_replytweets.json'.format(screen_name), 'r') as f:\n",
    "            replytweets = json.load(f)\n",
    "        print(\"Resuming from old alltweets file for {}.\".format(screen_name))\n",
    "        with open('{}_alltweets.json'.format(screen_name), 'r') as f:\n",
    "            alltweets = json.load(f)\n",
    "\n",
    "        if len(final_replytweets) != len(replytweets):\n",
    "            print(\"{} already collected all df_tweet.\".format(screen_name))\n",
    "            df_tweet = pd.read_csv('{}_df_tweet.csv'.format(screen_name))\n",
    "            print(\"Resuming from old alltweets file for {}.\".format(screen_name))\n",
    "            with open('{}_alltweets.json'.format(screen_name), 'r') as f:\n",
    "                alltweets = json.load(f)\n",
    "\n",
    "            counter = 0\n",
    "            big_counter = 0\n",
    "\n",
    "            #filter replies\n",
    "            print(\"{} filtering {} replytweets.\".format(screen_name, len(replytweets)))\n",
    "            final_replytweets = filter_replies(screen_name,alltweets,replytweets,final_replytweets,counter,big_counter)\n",
    "\n",
    "            #clean output\n",
    "    #                 print(\"{} Clearing duplicate final_replytweets.\".format(screen_name))\n",
    "    #                 final_replytweets = [i for n, i in enumerate(final_replytweets) if i not in final_replytweets[n + 1:]]\n",
    "            print(\"{} {} directed comment tweets downloaded. SAVING.\".format(screen_name,len(final_replytweets)))\n",
    "            with open(screen_name+'_final_replytweets.json', 'w') as f:\n",
    "                json.dump(final_replytweets, f)\n",
    "\n",
    "        elif len(final_replytweets) == len(replytweets):\n",
    "            print(\"{} finished collecting {} replies.\".format(screen_name,len(alltweets)))\n",
    "\n",
    "    #write DataFrame\n",
    "    print(\"{} Writing df_reply.\".format(screen_name))\n",
    "    df_reply = write_df(screen_name,final_replytweets)\n",
    "    print(\"{} Saving df_reply of length: {}.\".format(screen_name,len(df_reply)))\n",
    "    df_reply.to_json(screen_name+'_df_reply.json')\n",
    "    df_reply.to_csv(screen_name+'_df_reply.csv',index=False)\n",
    "    print(\"{} JSON and CSV files have been saved with {} replies. DONE!\".format(screen_name,len(df_reply)))\n",
    "    \n",
    "    #Deleting TweetScraper replies files\n",
    "    del_scraper(screen_name,password,main_dir,TweetScraper_path,saving_path_replies,files_path)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:11:43.838040Z",
     "start_time": "2019-12-23T22:08:37.791282Z"
    }
   },
   "outputs": [],
   "source": [
    "get_all(\"patriotact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:11:43.850043Z",
     "start_time": "2019-12-23T22:11:43.841500Z"
    }
   },
   "outputs": [],
   "source": [
    "# show_list = [\"TheDailyShow\",\"LastWeekTonight\",\"SouthPark\",\"nbcsnl\",\"colbertlateshow\",\"RealTimers\",\"TheOnion\",\n",
    "#             \"FullFrontalSamB\",\"JimmyKimmelLive\",\"LateNightSeth\",\"zondagmetlubach\",\"Lucky_TV\",\"AC360\",\"TuckerCarlson\",\n",
    "#             \"hardball\",\"CBSEveningNews\",\"11thHour\",\"NewsHour\",\"ABCWorldNews\",\"NightLine\",\"FaceTheNation\",\"60Minutes\",\n",
    "#             \"NBCNews\",\"MeetThePress\",\"NOS\",\"nosop3\",\"RTLnieuws\",\"Nieuwsuur\",\"patriotact\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:11:43.864297Z",
     "start_time": "2019-12-23T22:11:43.854113Z"
    }
   },
   "outputs": [],
   "source": [
    "# list(map(get_all, show_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:11:43.913362Z",
     "start_time": "2019-12-23T22:11:43.905600Z"
    }
   },
   "outputs": [],
   "source": [
    "# with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#     results = list(executor.map(get_all, show_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:11:43.956667Z",
     "start_time": "2019-12-23T22:11:43.917164Z"
    }
   },
   "outputs": [],
   "source": [
    "# list(map(get_all, show_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:11:43.979194Z",
     "start_time": "2019-12-23T22:11:43.963638Z"
    }
   },
   "outputs": [],
   "source": [
    "# screen_name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:11:43.996495Z",
     "start_time": "2019-12-23T22:11:43.990459Z"
    }
   },
   "outputs": [],
   "source": [
    "# get_all(screen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T22:11:44.022958Z",
     "start_time": "2019-12-23T22:11:44.006598Z"
    }
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     while True:\n",
    "#         engine9 = create_engine(\"mysql+pymysql://marksproject:IJKDEJFRknnkfr!!78278w2kjde@145.100.59.121/publicsphere?charset=utf8mb4\")\n",
    "#         con = engine9.connect()\n",
    "#         user_to_get_comments = con.execute(\"SELECT User FROM StatusControl WHERE Platform = 'Twitter' AND Comments = 'Pending' LIMIT 1\")\n",
    "#         user_to_get_comments = user_to_get_comments.fetchall()\n",
    "#         print(user_to_get_comments)\n",
    "#         if len(user_to_get_comments) > 0:\n",
    "#             screen_name = user_to_get_comments[0][0].replace('\\ufeff', '')\n",
    "#             print(screen_name)\n",
    "#             con.execute(\"UPDATE StatusControl SET Comments = 'Ongoing' WHERE User ='\"+screen_name+\"' AND Platform = 'Twitter'\")\n",
    "#             print('Started with', screen_name)\n",
    "# #             with open(screen_name+'_df_reply.json', 'r') as f:\n",
    "# #                 data = json.load(f)\n",
    "#             df_reply = pd.read_csv('{}_df_reply.csv'.format(screen_name))\n",
    "#             df_reply.to_sql('Twitter_Comments', con=con, index=False, if_exists='append')\n",
    "#             con.execute(\"UPDATE StatusControl SET Comments = 'Complete' WHERE User ='\"+screen_name+\"' AND Platform = 'Twitter'\")\n",
    "#         else:\n",
    "#             break"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
